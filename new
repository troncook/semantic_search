import os
import faiss
import sqlite3
import pdfplumber
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# === CONFIG ===
PDF_DIR = './pdfs'
DB_PATH = 'pdf_chunks.db'
INDEX_PATH = 'faiss.index'
CHUNK_SIZE = 300
TOP_K_REPORTS = 10
CHUNK_SEARCH_LIMIT = 100

model = SentenceTransformer('all-MiniLM-L6-v2')

# === UTILS ===
def extract_text_from_pdf(path):
    text = ''
    with pdfplumber.open(path) as pdf:
        for i, page in enumerate(pdf.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n[Page {i+1}]\n{page_text}"
    return text

def chunk_text(text, max_length=CHUNK_SIZE):
    paras = text.split('\n')
    chunks = []
    current = ''
    for p in paras:
        if len(current) + len(p) < max_length:
            current += ' ' + p
        else:
            chunks.append(current.strip())
            current = p
    if current:
        chunks.append(current.strip())
    return chunks

def initialize_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('DROP TABLE IF EXISTS chunks')
    c.execute('''
        CREATE TABLE chunks (
            id INTEGER PRIMARY KEY,
            file_name TEXT,
            chunk_id INTEGER,
            chunk_text TEXT
        )
    ''')
    conn.commit()
    return conn, c

def build_index():
    conn, c = initialize_db()
    all_embeddings = []

    print(f"🔍 Scanning PDFs in: {PDF_DIR}")
    for filename in tqdm(os.listdir(PDF_DIR)):
        if not filename.lower().endswith('.pdf'):
            continue
        path = os.path.join(PDF_DIR, filename)
        try:
            text = extract_text_from_pdf(path)
            chunks = chunk_text(text)
            embeddings = model.encode(chunks, convert_to_tensor=False)
            all_embeddings.extend(embeddings)
            for i, chunk in enumerate(chunks):
                c.execute("INSERT INTO chunks (file_name, chunk_id, chunk_text) VALUES (?, ?, ?)", (filename, i, chunk))
        except Exception as e:
            print(f"❌ Failed to process {filename}: {e}")

    conn.commit()
    conn.close()

    if all_embeddings:
        dim = len(all_embeddings[0])
        index = faiss.IndexFlatL2(dim)
        index.add(np.array(all_embeddings))
        faiss.write_index(index, INDEX_PATH)
        print(f"✅ FAISS index built and saved to {INDEX_PATH}")
    else:
        print("⚠️ No valid content to index.")

def search(query, top_k=TOP_K_REPORTS, chunk_limit=CHUNK_SEARCH_LIMIT):
    index = faiss.read_index(INDEX_PATH)
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()

    q_vec = model.encode([query], convert_to_tensor=False)
    D, I = index.search(np.array(q_vec), chunk_limit)

    file_scores = {}
    file_chunks = {}

    for score, idx in zip(D[0], I[0]):
        c.execute("SELECT file_name, chunk_id, chunk_text FROM chunks WHERE id=?", (idx + 1,))
        row = c.fetchone()
        if row:
            file = row[0]
            if file not in file_scores or score < file_scores[file]:
                file_scores[file] = score
                file_chunks[file] = {
                    "file": file,
                    "chunk_id": row[1],
                    "text": row[2],
                    "score": score
                }

    conn.close()

    top_files = sorted(file_chunks.values(), key=lambda x: x["score"])[:top_k]
    return top_files

# === CLI INTERFACE ===
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--build', action='store_true', help='Extract, chunk, and index all PDFs')
    parser.add_argument('--search', type=str, help='Search query')
    args = parser.parse_args()

    if args.build:
        build_index()
    elif args.search:
        results = search(args.search)
        for result in results:
            print(f"\n📄 {result['file']} (Chunk {result['chunk_id']})")
            print(f"Score: {result['score']:.4f}")
            print(result['text'])
            print("-" * 80)
    else:
        print("Usage:")
        print("  python semantic_pdf_search.py --build")
        print("  python semantic_pdf_search.py --search \"your query here\"")
